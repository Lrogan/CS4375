{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10ba09aa",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Forward propagation <br>\n",
    "layer_1 = Dense(units=3, activation='sigmoid') <br>\n",
    "layer_2 = Dense(units=1, activation='sigmoid') <br>\n",
    "x = np.array([200, 17])  \n",
    "\\begin{align*} \n",
    "w_{1}^{[1]} &= \\text{np.array([1,2])} & w_{1}^{[2]} &= \\text{np.array([-3,4])} & w_{1}^{[3]} &= \\text{np.array([5,-6])} \\newline \n",
    "b_{1}^{[1]} &= \\text{np.array([-1])} & b_{1}^{[2]} &= \\text{np.array([1])} & b_{1}^{[3]} &= \\text{np.array([2])} \\newline \n",
    "z_{1}^{[1]} &= {np.dot(w_{1}^{[1]}, x) + b_{1}^{[1]}}  & z_{1}^{[2]} &= {np.dot(w_{1}^{[2]}, x) + b_{1}^{[2]}} & z_{1}^{[3]} &= {np.dot(w_{1}^{[3]}, x) + b_{1}^{[3]}} \\newline \n",
    "a_{1}^{[1]} &= {sigmoid(z_{1}^{[1]})} & a_{1}^{[2]} &= {sigmoid(z_{1}^{[2]})} & a_{1}^{[3]} &= {sigmoid(z_{1}^{[3]})} \\newline \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "& a_{1} = np.array(a_{1}^{[1]}, a_{1}^{[2]}, a_{1}^{[3]}) \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b7b859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200.,  17.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[200.0, 17.0]]) # 1x2 matrix\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e9bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # or PyTorch\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c94bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1. 1. 0.]], shape=(1, 3), dtype=float32)\n",
      "[[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "layer_1 = Dense(units=3, activation='sigmoid') # 3 neurons\n",
    "a1 = layer_1(X)\n",
    "#tf.Tensor([[0.2 0.7 0.3]]), shape(1,3), dtype=float32) Tensor is a data type to representing matrix\n",
    "#a1.numpy() to reverse it\n",
    "print(a1)\n",
    "a1 = a1.numpy()\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698863a",
   "metadata": {},
   "source": [
    "## Forward Prop in NumPy\n",
    "W = np.array([1, -3, 5], <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2, 4, -6]])   2X3 <br>\n",
    "b = np.array([-1, 1, 2])  <br>\n",
    "a_in = np.array([-2, 4])  <br>\n",
    "\n",
    "def dense(a_in, W, b, g): <br>\n",
    "&emsp; &emsp;units = W.shape[1] <br>\n",
    "&emsp; &emsp;a_out = np.zeros(units) <br>\n",
    "&emsp; &emsp;for j in range(units): 0,1,2 <br>\n",
    "&emsp; &emsp;&emsp;w = W[:,j] <br>\n",
    "&emsp; &emsp;&emsp;z = np.dot(w, a_in) + b[j] <br>\n",
    "&emsp; &emsp;&emsp;a_out[j] = g(z) <br>\n",
    "&emsp; &emsp;return a_out\n",
    "   \n",
    "def sequential(x): <br>\n",
    "   &emsp; &emsp; a1 = dense(x, W1, b1) <br>\n",
    "   &emsp; &emsp; a2 = dense(a1, W2, b2) <br>\n",
    "   &emsp; &emsp; a3 = dense(a2, W3, b3) <br>\n",
    "   &emsp; &emsp; a4 = dense(a3, W4, b4) <br>\n",
    "   &emsp; &emsp; f_x = a4 <br>\n",
    "   &emsp; &emsp; return f_x\n",
    "   \n",
    "## Alternate approach\n",
    "def dense(A_in, W, B):<br>\n",
    "&emsp;Z = np.matmul(A_in, W) + B <br>\n",
    "&emsp;A_out = g(Z) <br>\n",
    "return A_out <br>\n",
    "\n",
    "a = [1  <br>\n",
    "&emsp;&emsp;2] <br>\n",
    "a_T = [1 2] <br>\n",
    "W = [3 5    <br>\n",
    "&emsp;&emsp; 4 6] <br>\n",
    "\n",
    "Z = a_T.W <br>\n",
    "Z = [11 17]\n",
    "\n",
    "Matrix Multiplicatio:  3 x 2 with 2 x 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ac50f",
   "metadata": {},
   "source": [
    "## Training \n",
    "<img align=\"left\" src=\"Training Steps - Andrew NG.png\"     style=\" width:1000px; padding: 10px; \" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccb4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # or PyTorch\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87da7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(units=25, activation='sigmoid'),\n",
    "        Dense(units=15, activation='sigmoid'),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0936da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "model.compile(loss=BinaryCrossentropy()) # for Binary Classification problem\n",
    "#model.fit(X, y, epochs=100) # epochs: number of steps in gradient descent using \"back propagation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c6bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=MeanSquaredError()) # for Regression problem\n",
    "#model.predict(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd637f",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "Sigmoid  0 < g(z) < 1 when y in <0, 1> <br>\n",
    "ReLU (Rectified Linear Unit): g(z) = max(0, z); if z<0 then g(z) is 0 else g(z) is Z when y <non negative values> <br>\n",
    "Linear Activiation Function: g(z) = z when y in <-1.2, 1.5, -0.4> don't use in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2438ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification\n",
    "model = Sequential([\n",
    "        Dense(units=25, activation='relu'),\n",
    "        Dense(units=15, activation='relu'),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fe6629",
   "metadata": {},
   "source": [
    "## Multi Class \n",
    "\n",
    "- Softmax regression (4 possible outputs) y = 1, 2, 3, 4 <br>\n",
    "&emsp; z1 = W1 . X + b1 <br>\n",
    "&emsp; a1 = e^z1 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=1|x) <br>\n",
    "\n",
    "&emsp; z2 = W2 . X + b2 <br>\n",
    "&emsp; a2 = e^z2 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=2|x) <br>\n",
    "\n",
    "&emsp; z1 = W1 . X + b1 <br>\n",
    "&emsp; a1 = e^z1 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=1|x) <br>\n",
    "\n",
    "&emsp; z1 = W1 . X + b1 <br>\n",
    "&emsp; a1 = e^z1 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=1|x) <br>\n",
    "\n",
    "Softmax Regression (N posssible outputs) <br>\n",
    "z_j = W_j . X + b_j  j=1,2,...,N <br>\n",
    "a_j = e^z_j / Sum(e^z_k) = P(y=j|X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee044814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000200000000000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1= 2.0 / 10000\n",
    "print(f\"{x1:.18f}\")\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb7c27",
   "metadata": {},
   "source": [
    "## Numerical Roundoff Errors\n",
    "\n",
    "### Numerically accurate implementation of logistic loss:\n",
    "\n",
    "#### Logistic regression:\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}\\tag{1}$\n",
    "$a = {g(z)}\\tag{2}$\n",
    "\n",
    "#### Original loss\n",
    "$loss = {-ylog(a) - (1-y)log(1-a)}\\tag{3}$\n",
    "\n",
    "#### More accurate loss\n",
    "$loss = {-ylog(\\frac{1}{1+e^{-z}}) - (1-y)log(1-\\frac{1}{1+e^{-z}})}\\tag{4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4396c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification\n",
    "model = Sequential([\n",
    "            Dense(units=25, activation='relu'),\n",
    "            Dense(units=15, activation='relu'),\n",
    "            Dense(units=10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Original Loss\n",
    "model.compile(loss=BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c823b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More accurate loss\n",
    "model.compile(loss=BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a07acd",
   "metadata": {},
   "source": [
    "## Numerical Roundoff Errors\n",
    "\n",
    "### Numerically accurate implementation of softmax:\n",
    "\n",
    "#### Softmax regression:\n",
    "\n",
    "$(a1,...,a10) = {g(z1,...,z10)}\\tag{1}$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  Loss = L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1) & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N) & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "#### More accurate loss\n",
    "\\begin{equation}\n",
    "  Loss = L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(\\frac{e^{-z_1}}{e^{-z_1}+e^{-z_2}+ ... + e^{-z_N}}) & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(\\frac{e^{-z_N}}{e^{-z_1}+e^{-z_2}+ ... + e^{-z_N}}) & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9fce04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification\n",
    "model = Sequential([\n",
    "            Dense(units=25, activation='relu'),\n",
    "            Dense(units=15, activation='relu'),\n",
    "            Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Original Loss\n",
    "model.compile(loss=SparseCategoricalCrossentropy())\n",
    "\n",
    "# More accurate\n",
    "model = Sequential([\n",
    "            Dense(units=25, activation='relu'),\n",
    "            Dense(units=15, activation='relu'),\n",
    "            Dense(units=10, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ae1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label Classification (Multiple outputs)\n",
    "# A self driving car with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc37253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When learning rate is too small in Gradient Descent, there is another lgorithm that takes faster steps\n",
    "# Adaptive Moement Estimation: Adam algorithm can adjust the alpha to larger or smaller learning rates automatically\n",
    "# it can assign a separate learning rate for each w and b\n",
    "# if w and b keeps moving in same direction, increase alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c9a2a",
   "metadata": {},
   "source": [
    "Adam: Adaptive Moment estimation (not just one alpha)\n",
    "$$\\begin{align*} \\; \\newline\\;\n",
    "& w_1 = w_1 -  \\alpha1 \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_1}   \\; \\newline\n",
    "& w_2 = w_2 -  \\alpha2 \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_2}   \\; \\newline\n",
    "& w_3 = w_3 -  \\alpha3 \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_3}   \\; \\newline\n",
    "&b\\ \\ = b -  \\alpha4 \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b7808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
